{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d45fd5",
   "metadata": {},
   "source": [
    "# Ml Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2a9749",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b19073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from components.data_manager import Data_Manager\n",
    "import components.network_data as nwd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "from os import cpu_count\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c8c42",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45f7552",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_DEFAULT_SIM_PATH: str = \"data/default_simulation/default_simulation_dataset.csv\"\n",
    "TABLE_PATH: str = \"graphics/tables\"\n",
    "RANDOM_SEED: int = 1137\n",
    "\n",
    "tmp_cores: int | None = cpu_count()\n",
    "CORES: int = 4 if tmp_cores is None else tmp_cores\n",
    "\n",
    "# strings to choice the section of dataset: \"all\", \"no_target\" or \"with_target\"\n",
    "DATASET_PARTITION: str = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a18a4d1",
   "metadata": {},
   "source": [
    "## Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b6593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm: Data_Manager = Data_Manager()\n",
    "dm.load_csv(CSV_DEFAULT_SIM_PATH)\n",
    "\n",
    "df: pd.DataFrame = dm.csv_to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20ecfaf",
   "metadata": {},
   "source": [
    "## Select the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PARTITION = DATASET_PARTITION.lower()\n",
    "\n",
    "# name of the results table\n",
    "TABLE_NAME: str = f\"{TABLE_PATH}/default_simulations-table-\"\n",
    "\n",
    "# all dataset\n",
    "if DATASET_PARTITION == \"all\":\n",
    "    TABLE_NAME += \"all_dataset.csv\"\n",
    "\n",
    "# without targets\n",
    "elif DATASET_PARTITION == \"no_target\":\n",
    "    df: pd.DataFrame = df.loc[df[nwd.TARGETS_PER_BLACK_HOLE] == 0]\n",
    "    TABLE_NAME += \"no_target.csv\"\n",
    "\n",
    "# with targets\n",
    "elif DATASET_PARTITION == \"with_target\":\n",
    "    df: pd.DataFrame = df.loc[\n",
    "        (df[nwd.TARGETS_PER_BLACK_HOLE] == 1) | (df[nwd.NUMBER_OF_BLACK_HOLES] == 0)\n",
    "    ]\n",
    "    TABLE_NAME += \"with_target.csv\"\n",
    "\n",
    "else:\n",
    "    raise Exception(\"No partition chosen.\")\n",
    "\n",
    "# to show numbers of simulations in dataset\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a0e0f",
   "metadata": {},
   "source": [
    "## Remove constants columns and answer columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    columns=[\n",
    "        # string column\n",
    "        nwd.INTENSITY,\n",
    "\n",
    "        # answer columns\n",
    "        nwd.NUMBER_OF_BLACK_HOLES,\n",
    "        nwd.BLACK_HOLE_SWAP_PROB,\n",
    "        nwd.TARGETS_PER_BLACK_HOLE,\n",
    "\n",
    "        # Constant columns\n",
    "        nwd.REQUESTS,\n",
    "        nwd.PARAMETER,\n",
    "        nwd.TOPOLOGY,\n",
    "        nwd.TOTAL_NO_PATHS,\n",
    "        nwd.NUMBER_OF_NODES,\n",
    "\n",
    "        # Redundant features\n",
    "        nwd.TOTAL_REQUEST_FAILS, # TOTAL_REQUEST_SUCCESS\n",
    "        nwd.TOTAL_SWAPPING_FAILS, # TOTAL_SWAPPING_FAILS\n",
    "\n",
    "        # feature don't worry\n",
    "        nwd.SIMULATION_TIME,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e04a94",
   "metadata": {},
   "source": [
    "## Function to create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c878aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_dict() -> dict:\n",
    "    return {\n",
    "        \"Random Forest\": RandomForestClassifier(62, n_jobs=CORES),\n",
    "        \"Gradient Boost\": GradientBoostingClassifier(),\n",
    "        \"SGD Classifier\": SGDClassifier(),\n",
    "        \"Benoulli NB\": BernoulliNB(),\n",
    "        \"MLP\": MLPClassifier(hidden_layer_sizes=(20, 30, 30), early_stopping=True, random_state=RANDOM_SEED),\n",
    "        \"Linear SVC\": LinearSVC(),\n",
    "        \"Gaussian NB\": GaussianNB()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22fb3a",
   "metadata": {},
   "source": [
    "## Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X: pd.DataFrame = df.drop(columns=nwd.ATTACK_TYPE)\n",
    "y: pd.Series = df[nwd.ATTACK_TYPE]\n",
    "\n",
    "mi_scores = mutual_info_classif(X, y)\n",
    "\n",
    "features_names = X.columns\n",
    "scores_series = pd.Series(mi_scores, index=features_names)\n",
    "scores_series = scores_series.sort_values(ascending=False)\n",
    "\n",
    "display(scores_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a79b46",
   "metadata": {},
   "source": [
    "## Cleaning the X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc2c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the features with lower mi score\n",
    "X = X.drop(columns=[\n",
    "    nwd.CONSUMED_EPRS,\n",
    "    nwd.TOTAL_ROUTE_LENGTH,\n",
    "    nwd.TOTAL_ENTANGLEMENT_ATTEMPTS,\n",
    "    nwd.TOTAL_SWAPPING_ATTEMPTS,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634c9f8b",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b74b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d5a17",
   "metadata": {},
   "source": [
    "## Function to fit all algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6434615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_algorithms(models: dict, X_train: pd.DataFrame, y_train: pd.Series) -> None:\n",
    "    for name, model in models.items():\n",
    "\n",
    "        start = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        print(f\"{name} fit time: {time.time() - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aefb82",
   "metadata": {},
   "source": [
    "## Fit all algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08628b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "models: dict = get_models_dict()\n",
    "\n",
    "fit_algorithms(models, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c686ea",
   "metadata": {},
   "source": [
    "## Collect Scores without Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118232ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity_score(y_true: pd.Series | np.ndarray, y_pred: pd.Series | np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    To analyze the True False percetage\n",
    "    \"\"\"\n",
    "\n",
    "    cm: np.ndarray = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    return TN / (TN + FP) if TN + FP != 0 else 0\n",
    "\n",
    "def predict_models_no_cross_validation(models: dict, X_test: pd.DataFrame, y_test: pd.Series) -> tuple[dict[str, dict], dict[str, np.ndarray]]:\n",
    "    data: dict = {}\n",
    "    confusion_matrices_models: dict = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"Model: {name}\")\n",
    "        start = time.time()\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        model_data: dict = {\n",
    "            \"f1 score\": f1_score(y_test, y_pred),\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision score\": precision_score(y_test, y_pred, zero_division=0),\n",
    "            \"recall score\": recall_score(y_test, y_pred),\n",
    "            \"specificity\": specificity_score(y_test, y_pred),\n",
    "            \"predict time\": (time.time()-start)\n",
    "        }\n",
    "\n",
    "        confusion_matrices_models[name] = confusion_matrix(y_test, y_pred)\n",
    "        data[name] = model_data\n",
    "\n",
    "    return data, confusion_matrices_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f379cf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, confusion_matrices = predict_models_no_cross_validation(models, X_test, y_test)\n",
    "\n",
    "print(\"Results\")\n",
    "for model_name, score in scores.items():\n",
    "    print(f\"{model_name}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1e4483",
   "metadata": {},
   "source": [
    "## Show confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920689a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = len(confusion_matrices)\n",
    "\n",
    "columns: int = 3\n",
    "rows: int = math.ceil(num_models / columns)\n",
    "\n",
    "fig, axes = plt.subplots(rows, columns, figsize=(5*columns, 4*rows))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (model_name, cm) in zip(axes, confusion_matrices.items()):\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=ax)\n",
    "    ax.set_title(model_name)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Real Value')\n",
    "    for ax in axes:\n",
    "        for _, spine in ax.spines.items():\n",
    "            spine.set_visible(True)\n",
    "            spine.set_linewidth(1.5)\n",
    "            spine.set_color('black')\n",
    "\n",
    "for i in range(num_models, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed1d6e",
   "metadata": {},
   "source": [
    "## Show result data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc9f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df: pd.DataFrame = pd.DataFrame(scores).T\n",
    "results_df.reset_index(inplace=True)\n",
    "results_df_fi_time = results_df['predict time']\n",
    "results_df = results_df.drop(columns='predict time')\n",
    "results_df.rename(columns={'index': 'Model'}, inplace=True)\n",
    "\n",
    "results_df_long = results_df.melt(id_vars=\"Model\", var_name=\"Metrics\", value_name=\"Value\")\n",
    "\n",
    "plt.figure(figsize=(10, 9))\n",
    "sns.barplot(data=results_df_long, x=\"Model\", y=\"Value\", hue=\"Metrics\")\n",
    "plt.title(\"Models' Performance\")\n",
    "plt.ylim(0, 1)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.grid(True, color=\"gray\", linestyle=\"--\", alpha=0.5, axis='y')\n",
    "plt.legend(title=\"Metrics\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfdc47d",
   "metadata": {},
   "source": [
    "## Make a cross validation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db374c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def predict_models_with_cross_validation(models: dict, X: pd.DataFrame, y: pd.Series) -> tuple[dict[str, dict], dict[str, float]]:\n",
    "    data: dict = {}\n",
    "    time_results: dict = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"Model: {name}\")\n",
    "        start = time.time()\n",
    "        \n",
    "        scoring = {\n",
    "            'accuracy': 'accuracy', \n",
    "            'f1_score': make_scorer(f1_score, average='macro'),\n",
    "            'precision_score': make_scorer(precision_score, zero_division=0),\n",
    "            'recall_score': make_scorer(recall_score),\n",
    "        }\n",
    "\n",
    "        results: dict = cross_validate(model, X, y, cv=5, scoring=scoring)\n",
    "\n",
    "        total_time = time.time() - start\n",
    "        print(f\"{name} cross time: {total_time}\")\n",
    "        \n",
    "        time_results[name] = total_time\n",
    "        data[name] = {test_name: result for test_name, result in results.items()}\n",
    "\n",
    "    return data, time_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae799a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = get_models_dict()\n",
    "results, time_results = predict_models_with_cross_validation(models, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cb248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score_dict: dict[str, dict] = {}\n",
    "\n",
    "for model_name, tmp_result in results.items():\n",
    "    result_mean: dict = {}\n",
    "\n",
    "    for name_result, array_result in tmp_result.items():\n",
    "        result_mean[name_result] = array_result.mean()\n",
    "        \n",
    "        \n",
    "    print(f\"{model_name}: {result_mean}\")\n",
    "    mean_score_dict[model_name] = result_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec7a4b3",
   "metadata": {},
   "source": [
    "## Show graphics from cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4ec056",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df: pd.DataFrame = pd.DataFrame(mean_score_dict).T\n",
    "results_df.reset_index(inplace=True)\n",
    "results_df_fi_time = results_df[\"fit_time\"]\n",
    "results_df = results_df.drop(columns=[\"fit_time\", \"score_time\"])\n",
    "results_df.rename(columns={\"index\": \"Model\", \"test_accuracy\": \"Accuracy\", \"test_f1_score\": \"F1\", \"test_precision_score\" : \"Precision\", \"test_recall_score\": \"Recall\"}, inplace=True)\n",
    "\n",
    "results_df_long = results_df.melt(id_vars=\"Model\", var_name=\"Metrics\", value_name=\"Value\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=results_df_long, x=\"Model\", y=\"Value\", hue=\"Metrics\")\n",
    "plt.title(\"Models' Performance\")\n",
    "plt.ylim(0, 1)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.grid(True, color=\"gray\", linestyle=\"--\", alpha=0.5, axis='y')\n",
    "plt.legend(title=\"Metrics\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30635ac",
   "metadata": {},
   "source": [
    "## Saving graphics of cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a3c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da5eb19",
   "metadata": {},
   "source": [
    "## Time Graphic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_name = list(time_results.keys())\n",
    "time_results = list(time_results.values())\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(models_name, time_results, color='skyblue')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Execution Time (s)')\n",
    "plt.title('Execution Time per Model')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
